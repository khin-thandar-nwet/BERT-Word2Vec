# Comparison of BERT and Word2Vec in Myanmar Language
** Word2Vec models generate embeddings that are context-independent: ie - there is just one vector (numeric) representation for each word. Different senses of the word (if any) are combined into one single vector.
** the BERT model generates embeddings that allow us to have multiple (more than one) vector (numeric) representations for the same word, based on the context in which the word is used. Thus, BERT embeddings are context-dependent.

# Environment
**I did all of this project on Google Colaboratory. Check it out in this link:

**Bert and Word2Vec  on Google Colab

